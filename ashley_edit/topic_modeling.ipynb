{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling of Solar Renewable Energy Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1a) Add DENA to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 name  year  \\\n",
      "0   Report_Status_and_Perspectives_of_Renewable_En...  2017   \n",
      "1   dena-Umfrage_Kuenstliche_Intelligenz_in_der_En...  2021   \n",
      "2         9265_dena_Kurzanalyse_Vermiedene_Netzkosten  2018   \n",
      "3                               HZwei_April_2020_dena  2020   \n",
      "4   9233_dena-Factsheet_Erneuerbare_Energien_und_d...    na   \n",
      "..                                                ...   ...   \n",
      "79  POSITIONSPAPIER_Zur_Fortsetzung_der_Mautbefrei...  2020   \n",
      "80            dena-ANALYSE_Marktmonitoring_Bioenergie  2021   \n",
      "81  Abschlussbericht_dena-Leitstudie_Aufbruch_Klim...  2021   \n",
      "82           POSITIONSPAPIER_EEG-Novelle_Layout_final  2021   \n",
      "83  dena-Abschlussbericht_Wirksamer_Klimaschutz_du...  2019   \n",
      "\n",
      "                                                  doc  BETD  DENA  news  \\\n",
      "0   Report \\nStatus and Perspectives of  \\nRenewab...     0     1     0   \n",
      "1   dena-UMFRAGE \\nKünstliche Intelligenz in der \\...     0     1     0   \n",
      "2   dena-KURZANALYSE  \\n\\nVermiedene Netzkosten \\n...     0     1     0   \n",
      "3   Hydrogeit Verlag / www.hzwei.info / 20. Jahrga...     0     1     0   \n",
      "4   H\\nb\\nm\\nG\\nt\\ns\\ni\\nr\\nb\\nO\\n©\\n\\nErneuerbare...     0     1     0   \n",
      "..                                                ...   ...   ...   ...   \n",
      "79  Foto: Iveco  \\n\\nPOSITIONSPAPIER \\nZur Fortset...     0     1     0   \n",
      "80  dena-ANALYSE \\nMarktmonitoring Bioenergie  \\nK...     0     1     0   \n",
      "81  Abschlussbericht\\ndena-Leitstudie  \\nAufbruch ...     0     1     0   \n",
      "82  POSITIONSPAPIER \\nBeschleunigung des EE-Ausbau...     0     1     0   \n",
      "83  dena-ABSCHLUSSBERICHT\\nWirksamer Klimaschutz \\...     0     1     0   \n",
      "\n",
      "   txt_sim_news txt_sim_fce txt_sim_proj  \n",
      "0            na          na           na  \n",
      "1            na          na           na  \n",
      "2            na          na           na  \n",
      "3            na          na           na  \n",
      "4            na          na           na  \n",
      "..          ...         ...          ...  \n",
      "79           na          na           na  \n",
      "80           na          na           na  \n",
      "81           na          na           na  \n",
      "82           na          na           na  \n",
      "83           na          na           na  \n",
      "\n",
      "[84 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Main objective for this cell: create and print out the dataframe with data from Kaggle.com\n",
    "# download the python packages you need and delete the ones you don't need\n",
    "\n",
    "import os # reads directory of stored DENA pdfs\n",
    "import re # regex is used for finding date published of DENA report\n",
    "\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text # reads pdf\n",
    "\n",
    "# todo - change this dataframe to fit the data that you download from Kaggle.com\n",
    "# todo - change the names of the data frame variables to fit your needs\n",
    "# todo - do you need both dataframes?\n",
    "df = pd.DataFrame(columns = [\"name\", \"year\", \"doc\", \"BETD\", \"DENA\", \"news\", \"txt_sim_news\", \"txt_sim_fce\", \"txt_sim_proj\"]) # todo - explain the code. Replace with answer\n",
    "betd_df = pd.DataFrame(columns = [\"name\", \"year\", \"doc\", \"BETD\", \"DENA\", \"news\", \"txt_sim_news\", \"txt_sim_fce\", \"txt_sim_proj\"]) # todo - explain the code. Replace with answer\n",
    "\n",
    "# todo - change the data to text data that you would like to analyze from Kaggle.com\n",
    "# todo - do you even need this next chunks of code? If no, delete and customize to your liking\n",
    "\n",
    "directory = 'DENA'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory,filename)\n",
    "    if os.path.isfile(file):\n",
    "        document = extract_text(file)\n",
    "        filename = filename.rstrip('.pdf')\n",
    "        reg_date = re.findall('2017|2018|2019|2020|2021|2022', document)\n",
    "\n",
    "        year = ''\n",
    "        if reg_date:\n",
    "            year  = reg_date[0]\n",
    "        if not reg_date:\n",
    "            reg_date = re.findall('2018|2019|2020|2021|2022', filename)\n",
    "            if reg_date:\n",
    "                year = reg_date[0]\n",
    "            if not reg_date:\n",
    "                year = 'na'\n",
    "        # insert to df\n",
    "        df.loc[-1] = [filename, year ,document, 0, 1, 0, 'na', 'na', 'na'] # todo - what does this code do? Replace this comment with answer\n",
    "        df.index = df.index + 1 # todo - what does this code do? Replace this comment with answer\n",
    "        df = df.sort_index() # todo - what does this code do? Replace this comment with answer\n",
    "\n",
    "print(df)\n",
    "# print(df['doc'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1b) Importing BETD files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# todo - do you need this code? Delete if not, keep if yes\n",
    "directories = ['BETD18', 'BETD19', 'BETD21', 'BETD22']\n",
    "year = 2018\n",
    "for directory in directories:\n",
    "    if year == 2020:\n",
    "        year = year + 1\n",
    "    for filename in os.listdir(directory):\n",
    "        file = os.path.join(directory,filename)\n",
    "        if os.path.isfile(file):\n",
    "            with open(file, 'r', errors='ignore') as f:\n",
    "                document = f.readlines()\n",
    "            filename = filename.rstrip('.pdf')\n",
    "\n",
    "            # insert to df\n",
    "            df.loc[-1] = [filename, year ,document, 1, 0, 0, 'na', 'na', 'na']\n",
    "            df.index = df.index + 1\n",
    "            df = df.sort_index()\n",
    "\n",
    "            betd_df.loc[-1] = [filename, year ,document, 1, 0, 0, 'na', 'na', 'na']\n",
    "            betd_df.index = betd_df.index + 1\n",
    "            betd_df = df.sort_index()\n",
    "    year = year + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1c) Importing newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   name  year  \\\n",
      "0                    Strommärkte_fressen_Entlastung_au  2022   \n",
      "1              Bei_Stillstand_droht_eine_Kettenreaktion  2022   \n",
      "2             Der_Brei_stinkt_aber_das_Kraftwerk_läuft  2022   \n",
      "3                   In_welchen_Arsch_wir_treten_müssen  2020   \n",
      "4               Wachsender_Druck_für_Energie_Importsto  2022   \n",
      "...                                                 ...   ...   \n",
      "1286  POSITIONSPAPIER_Zur_Fortsetzung_der_Mautbefrei...  2020   \n",
      "1287            dena-ANALYSE_Marktmonitoring_Bioenergie  2021   \n",
      "1288  Abschlussbericht_dena-Leitstudie_Aufbruch_Klim...  2021   \n",
      "1289           POSITIONSPAPIER_EEG-Novelle_Layout_final  2021   \n",
      "1290  dena-Abschlussbericht_Wirksamer_Klimaschutz_du...  2019   \n",
      "\n",
      "                                                    doc  BETD  DENA  news  \\\n",
      "0     Strommärkte fressen Entlastung auf\\n\\ntaz.die ...     0     0     1   \n",
      "1     Bei Stillstand droht eine Kettenreaktion\\n\\nBö...     0     0     1   \n",
      "2     Der Brei stinkt, aber das Kraftwerk läuft\\n\\nt...     0     0     1   \n",
      "3     In welchen Arsch wir treten müssen\\n\\ntaz.die ...     0     0     1   \n",
      "4     Wachsender Druck / für Energie-Importstopp\\n\\n...     0     0     1   \n",
      "...                                                 ...   ...   ...   ...   \n",
      "1286  Foto: Iveco  \\n\\nPOSITIONSPAPIER \\nZur Fortset...     0     1     0   \n",
      "1287  dena-ANALYSE \\nMarktmonitoring Bioenergie  \\nK...     0     1     0   \n",
      "1288  Abschlussbericht\\ndena-Leitstudie  \\nAufbruch ...     0     1     0   \n",
      "1289  POSITIONSPAPIER \\nBeschleunigung des EE-Ausbau...     0     1     0   \n",
      "1290  dena-ABSCHLUSSBERICHT\\nWirksamer Klimaschutz \\...     0     1     0   \n",
      "\n",
      "     txt_sim_news txt_sim_fce txt_sim_proj  \n",
      "0              na          na           na  \n",
      "1              na          na           na  \n",
      "2              na          na           na  \n",
      "3              na          na           na  \n",
      "4              na          na           na  \n",
      "...           ...         ...          ...  \n",
      "1286           na          na           na  \n",
      "1287           na          na           na  \n",
      "1288           na          na           na  \n",
      "1289           na          na           na  \n",
      "1290           na          na           na  \n",
      "\n",
      "[1291 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# todo - do you need this code? Delete if not, keep if yes\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "directory = 'newspapers'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory,filename)\n",
    "    if os.path.isfile(file):\n",
    "        try:\n",
    "            document = extract_text(file)\n",
    "            filename = filename.rstrip('.pdf')\n",
    "            reg_date = re.findall('2017|2018|2019|2020|2021|2022', document)\n",
    "\n",
    "            year = ''\n",
    "            if reg_date:\n",
    "                year  = reg_date[0]\n",
    "            # insert to df\n",
    "            df.loc[-1] = [filename, year ,document, 0, 0, 1, 'na', 'na', 'na']\n",
    "            df.index = df.index + 1\n",
    "            df = df.sort_index()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2) Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2a) Removing irrelevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# removing years outside boundaries\n",
    "df['year'] = df['year'].fillna(0)\n",
    "df = df[df.year != 0]\n",
    "df = df[df.year != 2017]\n",
    "df = df[df.year != 'na']\n",
    "\n",
    "betd_df['year'] = betd_df['year'].fillna(0)\n",
    "betd_df = betd_df[betd_df.year != 0]\n",
    "betd_df = betd_df[betd_df.year != 2017]\n",
    "betd_df = betd_df[betd_df.year != 'na']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2b) Removing noise with regex (Albrecht et al. 2020:95-97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like & to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like\n",
    "    text = re.sub(r'<[^<>]*>', ' ', str(text))\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', str(text))\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', str(text))\n",
    "    # standalone sequences of specials, matches  but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', str(text))\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', str(text))\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', str(text))\n",
    "    # removing DENA word\n",
    "    text = re.sub(r'dena', ' ', str(text))\n",
    "    # removing taz word\n",
    "    text = re.sub(r'taz', ' ', str(text))\n",
    "    # removing handelsblatt word\n",
    "    text = re.sub(r'handelsblatt', ' ', str(text))\n",
    "    # removing tageszeitung word\n",
    "    text = re.sub(r'tageszeitung', ' ', str(text))\n",
    "    # removing innnen word\n",
    "    # text = re.sub(r'innen', ' ', str(text))\n",
    "    # replacing aktien word with aktie\n",
    "    text = re.sub(r'aktien', 'aktie', str(text))\n",
    "    # removing abbildung word\n",
    "    text = re.sub(r'abbildung', ' ', str(text))\n",
    "    # removing de word\n",
    "    # text = re.sub(r'de', ' ', str(text))\n",
    "    # removing lsblatt word\n",
    "    text = re.sub(r'lsblatt', ' ', str(text))\n",
    "    # removing ah word\n",
    "    text = re.sub(r'ah', ' ', str(text))\n",
    "    # removing nke word\n",
    "    # text = re.sub(r'nke', ' ', str(text))\n",
    "    # removing han word\n",
    "    text = re.sub(r'han', ' ', str(text))\n",
    "    # removing al word\n",
    "    text = re.sub(r'al', ' ', str(text))\n",
    "    # removing dafur word\n",
    "    # text = re.sub(r'dafur', ' ', str(text))\n",
    "    # removing pro word\n",
    "    text = re.sub(r'pro', ' ', str(text))\n",
    "    # removing nr word\n",
    "    text = re.sub(r'nr', ' ', str(text))\n",
    "    # removing print word\n",
    "    text = re.sub(r'print', ' ', str(text))\n",
    "    # removing len word\n",
    "    text = re.sub(r'len', ' ', str(text))\n",
    "    # removing rs word\n",
    "    # text = re.sub(r'rs', ' ', str(text))\n",
    "    # removing for word\n",
    "    text = re.sub(r'for', ' ', str(text))\n",
    "    # removing la word\n",
    "    text = re.sub(r'la', ' ', str(text))\n",
    "    # removing bzw word\n",
    "    # text = re.sub(r'bzw', ' ', str(text))\n",
    "    # text = re.sub(r'nen', ' ', str(text))\n",
    "    # text = re.sub(r'rt', ' ', str(text))\n",
    "    # text = re.sub(r'mo', ' ', str(text))\n",
    "    # text = re.sub(r'vgl', ' ', str(text))\n",
    "    text = re.sub(r'the', ' ', str(text))\n",
    "    text = re.sub(r'and', ' ', str(text))\n",
    "    text = re.sub(r'of', ' ', str(text))\n",
    "    text = re.sub(r'to', ' ', str(text))\n",
    "    text = re.sub(r'for', ' ', str(text))\n",
    "    text = re.sub(r'is', ' ', str(text))\n",
    "    text = re.sub(r'as', ' ', str(text))\n",
    "    text = re.sub(r'la', ' ', str(text))\n",
    "    text = re.sub(r'are', ' ', str(text))\n",
    "    text = re.sub(r'on', ' ', str(text))\n",
    "    text = re.sub(r'be', ' ', str(text))\n",
    "    text = re.sub(r'with', ' ', str(text))\n",
    "    text = re.sub(r'from', ' ', str(text))\n",
    "    # text = re.sub(r'rs', ' ', str(text))\n",
    "    # text = re.sub(r'fin', ' ', str(text))\n",
    "    # text = re.sub(r'thg', ' ', str(text))\n",
    "    # text = re.sub(r'se', ' ', str(text))\n",
    "    text = re.sub(r'by', ' ', str(text))\n",
    "    # text = re.sub(r'et', ' ', str(text))\n",
    "    text = re.sub(r'that', ' ', str(text))\n",
    "    text = re.sub(r'this', ' ', str(text))\n",
    "    text = re.sub(r'or', ' ', str(text))\n",
    "    text = re.sub(r'un', ' ', str(text))\n",
    "    text = re.sub(r'les', ' ', str(text))\n",
    "    text = re.sub(r'can', ' ', str(text))\n",
    "    # text = re.sub(r'nr', ' ', str(text))\n",
    "    text = re.sub(r'son', ' ', str(text))\n",
    "    # text = re.sub(r'of', ' ', str(text))\n",
    "    # text = re.sub(r'verf', ' ', str(text))\n",
    "    # text = re.sub(r'nn', ' ', str(text))\n",
    "    # text = re.sub(r'mal', ' ', str(text))\n",
    "    # text = re.sub(r'abb', ' ', str(text))\n",
    "    # text = re.sub(r'il', ' ', str(text))\n",
    "    text = re.sub(r'Il', ' ', str(text))\n",
    "    # text = re.sub(r'mo', ' ', str(text))\n",
    "    # text = re.sub(r'rung', ' ', str(text))\n",
    "    # text = re.sub(r'rungen', ' ', str(text))\n",
    "    text = re.sub(r'Ils', ' ', str(text))\n",
    "    text = re.sub(r'ils', ' ', str(text))\n",
    "    # text = re.sub(r'ing', ' ', str(text))\n",
    "    text = re.sub(r'it', ' ', str(text))\n",
    "    # text = re.sub(r'Ing', ' ', str(text))\n",
    "    # text = re.sub(r'rzeuge', ' ', str(text))\n",
    "    # text = re.sub(r'anfor', ' ', str(text))\n",
    "    text = re.sub(r'\\n', '', str(text))\n",
    "    return text.strip()\n",
    "\n",
    "df['clean_doc'] = df['doc'].apply(clean)\n",
    "df['doc_impurity'] = df['clean_doc'].apply(impurity, min_len=20) # refers to % impurity of text (.09 means 9% are suspicious for text)\n",
    "\n",
    "betd_df['clean_doc'] = betd_df['doc'].apply(clean)\n",
    "betd_df['doc_impurity'] = betd_df['clean_doc'].apply(impurity, min_len=20) # refers to % impurity of text (.09 means 9% are suspicious for text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2c) Character normalization with textacy (Albrecht et al. 2020:98-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# standardizes and normalizes words like urls, hyphens, unicode, numbers, etc.\n",
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "if textacy.__version__ < '0.11':\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize_hyphenated_words(text)\n",
    "        text = tprep.normalize_quotation_marks(text)\n",
    "        text = tprep.normalize_unicode(text)\n",
    "        text = tprep.remove_accents(text)\n",
    "        text = re.sub(r'_url_', '', str(text))\n",
    "        return text\n",
    "\n",
    "else:\n",
    "    # adjusted to textacy 0.11\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize.hyphenated_words(text)\n",
    "        text = tprep.normalize.quotation_marks(text)\n",
    "        text = tprep.normalize.unicode(text)\n",
    "        text = tprep.remove.accents(text)\n",
    "        text = tprep.replace.emails(text)\n",
    "        text = tprep.replace.urls(text)\n",
    "        text = tprep.replace.numbers(text)\n",
    "        text = tprep.replace.phone_numbers(text)\n",
    "        text = tprep.replace.hashtags(text)\n",
    "        text = re.sub(r'_url_', '', str(text))\n",
    "        return text\n",
    "\n",
    "df['clean_doc'] = df['clean_doc'].map(normalize)\n",
    "\n",
    "betd_df['clean_doc'] = betd_df['clean_doc'].map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2d) Pattern-based data masking with textacy (Albrecht et al. 2020:99-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    replace_urls = textacy.preprocessing.replace_urls\n",
    "else:\n",
    "    replace_urls = textacy.preprocessing.replace.urls\n",
    "\n",
    "df['clean_doc'] = df['clean_doc'].map(replace_urls)\n",
    "\n",
    "betd_df['clean_doc'] = betd_df['clean_doc'].map(replace_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2e) Tokenization with nltk for German (Albrecht et al. 2020:102-103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "betd_df['tokens'] = betd_df['clean_doc'].map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3) Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3a) Separating the df into three sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josearroyoportillo/GitHub/master_thesis/venv/lib/python3.11/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis.sklearn\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "dena_df = df[df.DENA == 1]\n",
    "news_df = df[df.news == 1]\n",
    "\n",
    "dena_df.shape # sample size projects\n",
    "news_df.shape # sample size newspaper articles\n",
    "betd_df.shape # sample size FCE dialogue\n",
    "\n",
    "# for future depiction of topics\n",
    "# percent next to topics refers to percent contribution of word to topic\n",
    "def display_topics(model, features, no_top_words=30):\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df_topics = pd.DataFrame()\n",
    "    j = []\n",
    "    for i in range (30):\n",
    "        j.append(i+1)\n",
    "    df_topics['#'] = j\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        topic = int(topic) + 1\n",
    "        topic_word = \"Topic %01d\" % topic\n",
    "        temp_list = []\n",
    "        for i in range(0, no_top_words):\n",
    "            temp_list_item = \"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total))\n",
    "            temp_list.append(temp_list_item)\n",
    "        df_topics[topic_word] = temp_list\n",
    "\n",
    "    display(df_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3b) BETD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### LDA (Albrecht et al., 2020, pgs. 221-222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m W_lda_text_matrix \u001b[39m=\u001b[39m lda_text_model\u001b[39m.\u001b[39mfit_transform(count_text_vectors)\n\u001b[1;32m      6\u001b[0m H_lda_text_matrix \u001b[39m=\u001b[39m lda_text_model\u001b[39m.\u001b[39mcomponents_\n\u001b[0;32m----> 9\u001b[0m display_topics(lda_text_model, count_text_vectorizer\u001b[39m.\u001b[39;49mget_feature_names())\n\u001b[1;32m     10\u001b[0m W_lda_text_matrix\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m/\u001b[39mW_lda_text_matrix\u001b[39m.\u001b[39msum()\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# implementation of LDA with visualization\n",
    "count_text_vectorizer = CountVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(betd_df['clean_doc'])\n",
    "lda_text_model = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_\n",
    "\n",
    "\n",
    "display_topics(lda_text_model, count_text_vectorizer.get_feature_names())\n",
    "W_lda_text_matrix.sum(axis=0)/W_lda_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.sklearn.prepare(lda_text_model, count_text_vectors, count_text_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Time development of models (Albrecht et al. 2020:228-229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "W_lda_text_matrix.sum(axis=0)/W_lda_text_matrix.sum()*100.0\n",
    "\n",
    "betd_df['year'] = betd_df['year'].fillna(0)\n",
    "betd_df['year'] = betd_df['year'].astype(int)\n",
    "betd_df = betd_df[betd_df.year != 0]\n",
    "betd_df = betd_df[betd_df.year != 2017]\n",
    "\n",
    "\n",
    "year_data = []\n",
    "\n",
    "for year in tqdm(np.unique(np.unique(betd_df[\"year\"]))):\n",
    "    W_year = lda_text_model.transform(count_text_vectors[np.array(betd_df[\"year\"] == year)])\n",
    "    year_data.append([year] + list(W_year.sum(axis=0)/W_year.sum()*100.0))\n",
    "\n",
    "topic_names = []\n",
    "voc = count_text_vectorizer.get_feature_names()\n",
    "\n",
    "j = 1\n",
    "for topic in lda_text_model.components_:\n",
    "    important = topic.argsort()\n",
    "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "    # topic_names.append(\"Topic \" + top_word)\n",
    "    topic_names.append(\"Topic \" + str(j))\n",
    "    j = int(j) + 1\n",
    "\n",
    "\n",
    "betd_df_year = pd.DataFrame(year_data, columns=[\"year\"] + topic_names).set_index(\"year\")\n",
    "print(betd_df_year)\n",
    "\n",
    "[f'Topic {count_text_vectorizer.get_feature_names()[words.argsort()[-1]]}' for words in lda_text_model.components_]\n",
    "\n",
    "betd_df_year.plot(title='Development of Field Narratives over Time', grid=True, use_index=True, subplots=False,\n",
    "                  ylabel='Percent of Total Terms', xlabel='Year', table=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# betd_df_year = betd_df_year.loc[:, [\"Topic 1\", \"Topic 2\",\"Topic 5\",\"Topic 9\"]]\n",
    "display(betd_df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3c) DENA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### LDA (Albrecht et al. 2020:221-222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# implementation of LDA with visualization\n",
    "count_text_vectorizer = CountVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(dena_df['clean_doc'])\n",
    "lda_text_model = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_\n",
    "\n",
    "\n",
    "display_topics(lda_text_model, count_text_vectorizer.get_feature_names())\n",
    "W_lda_text_matrix.sum(axis=0)/W_lda_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.sklearn.prepare(lda_text_model, count_text_vectors, count_text_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Time development of models (Albrecht et al. 2020:228-229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "W_lda_text_matrix.sum(axis=0)/W_lda_text_matrix.sum()*100.0\n",
    "\n",
    "dena_df['year'] = dena_df['year'].fillna(0)\n",
    "dena_df['year'] = dena_df['year'].astype(int)\n",
    "dena_df = dena_df[dena_df.year != 0]\n",
    "dena_df = dena_df[dena_df.year != 2017]\n",
    "\n",
    "\n",
    "year_data = []\n",
    "\n",
    "for year in tqdm(np.unique(np.unique(dena_df[\"year\"]))):\n",
    "    W_year = lda_text_model.transform(count_text_vectors[np.array(dena_df[\"year\"] == year)])\n",
    "    year_data.append([year] + list(W_year.sum(axis=0)/W_year.sum()*100.0))\n",
    "\n",
    "topic_names = []\n",
    "voc = count_text_vectorizer.get_feature_names()\n",
    "\n",
    "j = 1\n",
    "for topic in lda_text_model.components_:\n",
    "    important = topic.argsort()\n",
    "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "    # topic_names.append(\"Topic \" + top_word)\n",
    "    topic_names.append(\"Topic \" + str(j))\n",
    "    j = int(j) + 1\n",
    "\n",
    "\n",
    "dena_df_year = pd.DataFrame(year_data, columns=[\"year\"] + topic_names).set_index(\"year\")\n",
    "print(dena_df_year)\n",
    "\n",
    "[f'Topic {count_text_vectorizer.get_feature_names()[words.argsort()[-1]]}' for words in lda_text_model.components_]\n",
    "\n",
    "dena_df_year.plot(title='Development of Project Narratives over Time', grid=True, use_index=True, subplots=False,\n",
    "                  ylabel='Percent of Total Terms', xlabel='Year', table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# dena_df_year = dena_df_year.loc[:, [\"Topic 1\", \"Topic 4\",\"Topic 5\",\"Topic 7\"]]\n",
    "display(dena_df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3d) Newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### LDA (Albrecht et al. 2020, pg. 221-222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# implementation of LDA with visualization\n",
    "count_text_vectorizer = CountVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(news_df['clean_doc'])\n",
    "lda_text_model = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_\n",
    "\n",
    "\n",
    "display_topics(lda_text_model, count_text_vectorizer.get_feature_names())\n",
    "W_lda_text_matrix.sum(axis=0)/W_lda_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.sklearn.prepare(lda_text_model, count_text_vectors, count_text_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Time development of models (Albrecht et al. 2020:228-229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "W_lda_text_matrix.sum(axis=0)/W_lda_text_matrix.sum()*100.0\n",
    "\n",
    "news_df['year'] = news_df['year'].fillna(0)\n",
    "news_df['year'] = news_df['year'].astype(int)\n",
    "news_df = news_df[news_df.year != 0]\n",
    "news_df = news_df[news_df.year != 2017]\n",
    "\n",
    "\n",
    "year_data = []\n",
    "\n",
    "for year in tqdm(np.unique(np.unique(news_df[\"year\"]))):\n",
    "    W_year = lda_text_model.transform(count_text_vectors[np.array(news_df[\"year\"] == year)])\n",
    "    year_data.append([year] + list(W_year.sum(axis=0)/W_year.sum()*100.0))\n",
    "\n",
    "topic_names = []\n",
    "voc = count_text_vectorizer.get_feature_names()\n",
    "\n",
    "j = 1\n",
    "for topic in lda_text_model.components_:\n",
    "    important = topic.argsort()\n",
    "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "    # topic_names.append(\"Topic \" + top_word)\n",
    "    topic_names.append(\"Narr. \" + str(j))\n",
    "    j = int(j) + 1\n",
    "\n",
    "\n",
    "news_df_year = pd.DataFrame(year_data, columns=[\"year\"] + topic_names).set_index(\"year\")\n",
    "print(news_df_year)\n",
    "\n",
    "[f'Topic {count_text_vectorizer.get_feature_names()[words.argsort()[-1]]}' for words in lda_text_model.components_]\n",
    "\n",
    "news_df_year.plot(title='Development of Societal Narratives over Time', grid=True, use_index=True, subplots=False,\n",
    "                  ylabel='Percent of Total Terms', xlabel='Year', table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "display(news_df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1c0bcdb985bf5e9c8ec3075be2d12e4c38a6ff8e6f4c486cc0cee0e029db511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
